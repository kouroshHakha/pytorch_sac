{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d8ba3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace = /home/kourosh/projects/dm_sac/osil_notebooks\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "print(f'workspace = {Path.cwd()}')\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(Path('..').resolve().absolute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabaa83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kourosh/anaconda3/envs/urlb/lib/python3.8/site-packages/glfw/__init__.py:906: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "import dmc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "\n",
    "TASK_NAME = 'walker_stand'\n",
    "OBS_TYPE = 'pixels'\n",
    "BLOCK_SIZE = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DMC(Dataset):\n",
    "\n",
    "    def __init__(self, task_name='walker_stand', obs_type='state', block_size=64):\n",
    "\n",
    "        assert obs_type in ['state', 'pixels']\n",
    "\n",
    "        \n",
    "        self.block_size = block_size\n",
    "\n",
    "        with open(Path(root) / 'agent_runs' / f'{task_name}.pickle', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        self.obs = data['obs'] if obs_type == 'pixels' else data['state']\n",
    "        self.acs = data['action']\n",
    "        self.terminal = data['terminal']\n",
    "        self.reward = data['reward']\n",
    "\n",
    "        self.num_trajs = len(self.obs)\n",
    "        self.num_cols = len(self.obs[0]) - self.block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_trajs * self.num_cols\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "\n",
    "        traj_idx = idx // self.num_cols\n",
    "        start_idx = idx % self.num_cols\n",
    "        obses = self.obs[traj_idx][start_idx:start_idx + self.block_size]\n",
    "        acs = self.acs[traj_idx][start_idx:start_idx + self.block_size]\n",
    "        \n",
    "        x = torch.tensor(obses, dtype=torch.float)\n",
    "        y = torch.tensor(acs, dtype=torch.float)\n",
    "        return x, y\n",
    "\n",
    "dataset = DMC(task_name=TASK_NAME, obs_type=OBS_TYPE, block_size=BLOCK_SIZE)\n",
    "\n",
    "loader = DataLoader(dataset, shuffle=True, pin_memory=True,\n",
    "                                batch_size=256,\n",
    "                                num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1000, 3, 84, 84)\n",
      "99900\n",
      "x  torch.Size([1, 3, 84, 84])\n",
      "y  torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.obs.shape)\n",
    "print(len(dataset))\n",
    "x, y = dataset[len(dataset)-1]\n",
    "print('x ', x.shape)\n",
    "print('y ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bde0c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, obs_shape, h_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(obs_shape) == 3\n",
    "        self.repr_dim = 32 * 35 * 35\n",
    "\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv2d(obs_shape[0], 32, 3, stride=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(32, 32, 3, stride=1),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(32, 32, 3, stride=1),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(32, 32, 3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.lin = nn.Linear(self.repr_dim, h_dim, bias=True)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"Custom weight init for Conv2D and Linear layers.\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.orthogonal_(m.weight.data)\n",
    "            if hasattr(m.bias, 'data'):\n",
    "                m.bias.data.fill_(0.0)\n",
    "        elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            gain = nn.init.calculate_gain('relu')\n",
    "            nn.init.orthogonal_(m.weight.data, gain)\n",
    "            if hasattr(m.bias, 'data'):\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        obs = obs / 255.0 - 0.5\n",
    "        h = self.convnet(obs)\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        h = self.lin(h)\n",
    "        return h\n",
    "\n",
    "class BCMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h_dim = h_dim = conf.h_dim\n",
    "        self.obs_shape = obs_shape = conf.obs_dim\n",
    "        self.ac_dim = ac_dim = conf.ac_dim\n",
    "        \n",
    "        self.emb = None\n",
    "        if len(obs_shape) > 1:\n",
    "            self.emb = Encoder(obs_shape, h_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(nn.Linear(h_dim if self.emb else obs_shape[0], h_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(h_dim, h_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(h_dim, h_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(h_dim, ac_dim)\n",
    "                                )\n",
    "                \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.emb:\n",
    "            x = self.emb(x)\n",
    "        pred_ac = self.mlp(x)\n",
    "        return pred_ac\n",
    "    \n",
    "    def bc_loss(self, pred_ac, target_ac):\n",
    "     \n",
    "        pred_ac = pred_ac.view(-1, pred_ac.shape[-1])\n",
    "        target_ac = target_ac.view(-1, target_ac.shape[-1])\n",
    "        loss = F.mse_loss(pred_ac, target_ac)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "class GPTConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "\n",
    "    def __init__(self, vocab_size=2, block_size=64, \n",
    "                 n_layer=4, n_head=8, n_embd=512, obs_dim=4, ac_dim=2):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.n_layer=n_layer\n",
    "        self.n_head=n_head\n",
    "        self.h_dim=n_embd \n",
    "        \n",
    "        self.in_dim = vocab_size\n",
    "        self.out_dim = vocab_size\n",
    "        self.max_T = block_size\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.ac_dim = ac_dim\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d8055d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x, y = dataset[0]\n",
    "conf = GPTConfig(block_size=1, n_embd=256, obs_dim=x.shape[1:], ac_dim=y.shape[-1])\n",
    "\n",
    "\n",
    "weight_decay=1e-4\n",
    "learning_rate=1e-4\n",
    "betas=(0.99,0.999)\n",
    "device = torch.device('cuda')\n",
    "losses = []\n",
    "epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06627b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "model = BCMLP(conf)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=betas)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "if len(model.obs_shape) > 1:\n",
    "    x = torch.randn((3, 3, 84, 84)).float().to(device)\n",
    "    y = model(x)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 5/391 [00:00<00:19, 19.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45781368017196655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:14<00:00, 27.74it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 25.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3838927745819092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.46it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 26.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37194332480430603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.09it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 26.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35873112082481384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.42it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 25.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.349597692489624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.27it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 26.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3362366557121277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.81it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 26.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3426046371459961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.92it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 27.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3259008526802063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.28it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 27.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30183184146881104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 29.21it/s]\n",
      "  2%|▏         | 7/391 [00:00<00:13, 27.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3114248514175415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.94it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 27.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3128284811973572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.91it/s]\n",
      "  2%|▏         | 7/391 [00:00<00:13, 27.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2982085347175598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.43it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 27.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3044389486312866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.48it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 26.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28962451219558716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.68it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 27.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28639060258865356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 29.21it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 26.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29265156388282776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.56it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 27.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2804614007472992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 29.00it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 26.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26953691244125366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.74it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 27.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2649284899234772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 29.14it/s]\n",
      "  2%|▏         | 6/391 [00:00<00:14, 26.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2789513170719147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:13<00:00, 28.72it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "\n",
    "    for it, (x, y) in pbar:\n",
    "\n",
    "        # place data on the correct device\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "\n",
    "        # forward the model\n",
    "        with torch.set_grad_enabled(True):\n",
    "            x = x.squeeze(1)\n",
    "            y = y.squeeze(1)\n",
    "            pred_ac = model.forward(x)\n",
    "\n",
    "            loss = model.bc_loss(pred_ac, y)\n",
    "            loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "            losses.append(loss.item())\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if it % 1000 == 0:\n",
    "            print(loss.item())\n",
    "\n",
    "        #if it == 5000:\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6767d240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9a651dbcd0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e421e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Kernel is dead",
     "output_type": "error",
     "traceback": [
      "Error: Kernel is dead",
      "at g._sendKernelShellControl (/home/kourosh/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:1006305)",
      "at g.sendShellMessage (/home/kourosh/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:1006074)",
      "at g.requestExecute (/home/kourosh/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:1008616)",
      "at d.requestExecute (/home/kourosh/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:37:328037)",
      "at S.requestExecute (/home/kourosh/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:32:19306)",
      "at w.executeCodeCell (/home/kourosh/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:300924)",
      "at w.execute (/home/kourosh/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:300551)",
      "at w.start (/home/kourosh/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:296215)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/kourosh/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:310950)",
      "at async t.CellExecutionQueue.start (/home/kourosh/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:310490)"
     ]
    }
   ],
   "source": [
    "env = dmc.make(TASK_NAME, seed=10, obs_type=OBS_TYPE)\n",
    "pl.seed_everything(10)\n",
    "\n",
    "\n",
    "timestep = env.reset()\n",
    "print(timestep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a8cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': 10,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': np.mean(losses[-100:])\n",
    "            }, './bc.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30779c29",
   "metadata": {},
   "source": [
    "# Transfromer-based BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_head = conf.n_head\n",
    "        h_dim = conf.h_dim\n",
    "        max_T = conf.max_T\n",
    "        self.qnet = nn.Linear(h_dim, h_dim)\n",
    "        self.knet = nn.Linear(h_dim, h_dim)\n",
    "        self.vnet = nn.Linear(h_dim, h_dim)\n",
    "        \n",
    "        self.proj = nn.Linear(h_dim, h_dim)\n",
    "        \n",
    "        mask = torch.tril(torch.ones(max_T,max_T)).view(1,1,max_T,max_T)\n",
    "        self.register_buffer('mask',mask)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is (B, T, C)\n",
    "        assert x is not None\n",
    "        B, T, C = x.shape\n",
    "        N = self.n_head\n",
    "        H = C // N\n",
    "        assert C % N == 0\n",
    "        \n",
    "        q = self.qnet(x).view(B, T, N, H).transpose(1, 2)\n",
    "        k = self.knet(x).view(B, T, N, H).transpose(1, 2)\n",
    "        v = self.vnet(x).view(B, T, N, H).transpose(1, 2)\n",
    "        \n",
    "        att = q @ k.transpose(2, 3) * (1.0 / math.sqrt(H)) # (:, :, T, T)\n",
    "        \n",
    "        att.masked_fill(self.mask == 0, float('-inf'))\n",
    "        \n",
    "        att = F.softmax(att, dim=-1) # stays # (:, :, T, T)\n",
    "        \n",
    "        att = att @ v # (B, N, T, T) @ (B, N, T, H)\n",
    "        \n",
    "        # gather heads\n",
    "        \n",
    "        out = att.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        h_dim = config.h_dim\n",
    "        self.attention = Attention(config)\n",
    "        self.ln1 = nn.LayerNorm(h_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(h_dim, 4*h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*h_dim, h_dim),\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(h_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.attention(x)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.mlp(x)\n",
    "        x = self.ln2(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        super().__init__()\n",
    "\n",
    "        # token embeddings\n",
    "        vocab_dim = conf.in_dim\n",
    "        h_dim = conf.h_dim\n",
    "        obs_dim = conf.obs_dim\n",
    "        ac_dim = conf.ac_dim\n",
    "        self.obs_dim = obs_dim\n",
    "                \n",
    "        self.token_embed = nn.Linear(vocab_dim, h_dim)\n",
    "        \n",
    "        self.pos_encoding = nn.Parameter(torch.zeros(1, conf.max_T, h_dim))\n",
    "        \n",
    "        # transformer\n",
    "        self.transformer = nn.Sequential(*[Transformer(conf) for _ in range(conf.n_layer)])\n",
    "        self.ln = nn.LayerNorm(h_dim)\n",
    "        \n",
    "        # prediction head\n",
    "        self.pred_head = nn.Linear(h_dim, vocab_dim, bias=False)\n",
    "        \n",
    "        # policy head\n",
    "        task_dim = 16\n",
    "        self.task_proj = nn.Sequential(nn.Linear(h_dim, task_dim), nn.ReLU())\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(task_dim + obs_dim, 4*task_dim),nn.ReLU(), nn.Linear(4*task_dim, ac_dim))\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        B, T, C = x.shape\n",
    "        token_h = self.token_embed(x)\n",
    "        pos_h = self.pos_encoding[:, :T, :]\n",
    "        \n",
    "        h = token_h + pos_h\n",
    "\n",
    "        h = self.transformer(h)\n",
    "        \n",
    "        logits = self.pred_head(h)\n",
    "                \n",
    "        return logits, h\n",
    "    \n",
    "    def behavior_clone(self, z, x, y):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        diag = torch.arange(T)\n",
    "        \n",
    "    \n",
    "        \n",
    "        obs = x[:,diag,:self.obs_dim]\n",
    "        target_ac = y[:,diag, self.obs_dim:]\n",
    "        \n",
    "        z = self.task_proj(z)\n",
    "        \n",
    "        z_obs = torch.cat([obs, z], -1)\n",
    "      \n",
    "        pred_ac = self.policy_head(z_obs)\n",
    "        return pred_ac, target_ac\n",
    "    \n",
    " \n",
    "    def bc_loss(self, pred_ac, target_ac):\n",
    "     \n",
    "        pred_ac = pred_ac.view(-1, pred_ac.shape[-1])\n",
    "        target_ac = target_ac.view(-1, target_ac.shape[-1])\n",
    "        loss = F.mse_loss(pred_ac, target_ac)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def compute_loss(self, logits, targets):\n",
    "        B, T, C = logits.shape\n",
    "      \n",
    "        \n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        targets = targets.view(-1, targets.shape[-1])\n",
    "        loss = F.mse_loss(logits, targets)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "class BCMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        super().__init__()\n",
    "\n",
    "        h_dim = conf.h_dim\n",
    "        obs_dim = conf.obs_dim\n",
    "        ac_dim = conf.ac_dim\n",
    "        self.obs_dim = obs_dim\n",
    "        \n",
    "        self.mlp = nn.Sequential(nn.Linear(obs_dim, h_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(h_dim, h_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(h_dim, h_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(h_dim, ac_dim))\n",
    "                \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        \"\"\"\n",
    "        What we want here a_t_pred = f(o_{t-k}, a_{t-k}, ..., o_{t-1}, a_{t}, o_{t})\n",
    "        \"\"\"\n",
    "        \n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B*T, C)\n",
    "        obs = x[:,:self.obs_dim]\n",
    "        target_ac = x[:,self.obs_dim:]\n",
    "        \n",
    "        pred_ac = self.mlp(obs)\n",
    "                \n",
    "        return pred_ac, target_ac\n",
    "    \n",
    "    \n",
    "    def bc_loss(self, pred_ac, target_ac):\n",
    "     \n",
    "        pred_ac = pred_ac.view(-1, pred_ac.shape[-1])\n",
    "        target_ac = target_ac.view(-1, target_ac.shape[-1])\n",
    "        loss = F.mse_loss(pred_ac, target_ac)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def compute_loss(self, *args, **kwargs):\n",
    "        return 0.0\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76fec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm \n",
    "\n",
    "class GPTConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    \n",
    "    \n",
    "\n",
    "    def __init__(self, vocab_size=2, block_size=64, \n",
    "                 n_layer=4, n_head=8, n_embd=512, obs_dim=4, ac_dim=2):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.n_layer=n_layer\n",
    "        self.n_head=n_head\n",
    "        self.h_dim=n_embd \n",
    "        \n",
    "        self.in_dim = vocab_size\n",
    "        self.out_dim = vocab_size\n",
    "        self.max_T = block_size\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.ac_dim = ac_dim\n",
    "     \n",
    "       \n",
    "\n",
    "\n",
    "conf = GPTConfig(6, 64,\n",
    "                  n_layer=2, n_head=8, n_embd=256, obs_dim=4, \n",
    "                  )\n",
    "model = GPT(conf)\n",
    "\n",
    "weight_decay=1e-4\n",
    "learning_rate=1e-4\n",
    "betas=(0.99,0.999)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=betas)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "losses = []\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm.tqdm(enumerate(loader), total=len(loader)) if True else enumerate(loader)\n",
    "\n",
    "    for it, (x, y) in pbar:\n",
    "\n",
    "        # place data on the correct device\n",
    "        x = x.to(device).float()\n",
    "\n",
    "        y = y.to(device).float()\n",
    "\n",
    "        # forward the model\n",
    "        with torch.set_grad_enabled(True):\n",
    "            x = x.view(-1,x.shape[-1])\n",
    "            y = y.view(-1, y.shape[-1])\n",
    "            pred_ac = model.forward(x)\n",
    "\n",
    "            loss = model.bc_loss(pred_ac, y)\n",
    "            loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "            losses.append(loss.item())\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if it % 1000 == 0:\n",
    "            print(loss.item())\n",
    "\n",
    "        #if it == 5000:\n",
    "        #    break\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f49e71d5a4561ebae520361f20f6b09ea77b1ad6de91bf873e838bdff1a7e76"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('urlb': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
